{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shanghong/miniconda3/envs/cot/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hello world\", \"How are you?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000,   9906,   1917, 128002, 128002],\n",
      "        [128000,   4438,    527,    499,     30]]), 'attention_mask': tensor([[1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(\"<|reserved_special_token_0|>\")\n",
    "tokens = [tokenizer(t) for t in texts]\n",
    "\n",
    "# Default collate function \n",
    "collate_fn = DataCollatorWithPadding(tokenizer, padding=True) #padding=True, 'max_length'\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=2) \n",
    "for batch in dataloader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
      "          1012,   102],\n",
      "        [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n",
      "           102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'labels': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
      "          1012,   102],\n",
      "        [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n",
      "           102,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "texts = [\n",
    "  \"The quick brown fox jumps over the lazy dog.\",\n",
    "  \"I am learning about NLP and AI today\"  \n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokens = [tokenizer(t) for t in texts]\n",
    "\n",
    "collate_fn = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=tokens, collate_fn=collate_fn, batch_size=2)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
       "          1012,   102],\n",
       "        [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n",
       "           102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'labels': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
       "          1012,   102],\n",
       "        [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n",
       "           102,  -100]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_fn(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "import math\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score  # This will no longer be used but kept for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshanghong_sim\u001b[0m (\u001b[33mstlm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shanghong/work/CoT-Hackathon/train_value_model/wandb/run-20241013_122326-9t8200m9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/stlm/COT/runs/9t8200m9' target=\"_blank\">Value Model: Llama-3.2-1B-Instruct-LM</a></strong> to <a href='https://wandb.ai/stlm/COT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/stlm/COT' target=\"_blank\">https://wandb.ai/stlm/COT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/stlm/COT/runs/9t8200m9' target=\"_blank\">https://wandb.ai/stlm/COT/runs/9t8200m9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128257, 2048)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Initialize WandB\n",
    "wandb.init(\n",
    "    project=\"COT\",\n",
    "    name=\"Value Model: Llama-3.2-1B-Instruct-LM\"\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Ensure this is the correct model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Initialize the model for causal language modeling\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Add the special tokens to the tokenizer\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': [\n",
    "        '<|reserved_special_token_10|>',\n",
    "        '<|reserved_special_token_11|>',\n",
    "        '<|reserved_special_token_12|>',\n",
    "        '<|reserved_special_token_13|>',\n",
    "        '[PAD]'\n",
    "    ]\n",
    "}\n",
    "# Add pad token\n",
    "tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.pad_token = '[PAD]'\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # Assuming pad token is same as eos\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 911716/911716 [01:31<00:00, 9921.72 examples/s] \n",
      "Map: 100%|██████████| 9210/9210 [00:01<00:00, 8278.97 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Define the dataset and tokenization function\n",
    "dataset = load_dataset(\"LeonGuertler/PRM800K_train2_updated\")\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.01)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,          # Optional: Set max_length to control padding\n",
    "        padding=False            # Let the data collator handle padding\n",
    "    )\n",
    "    tokenized_inputs[\"labels\"] = examples[\"value_label\"]  # Assign labels correctly\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Initialize DataCollatorWithPadding for dynamic padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'value_label', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 911716\n",
      "})\n",
      "----\n",
      "{'text': ['We realize that $a^3+b^3$ is the sum of two cubes and thus can be expressed as $(a+b)(a^2-ab+b^2)$. From this, we have  \\\\begin{align*}\\na^3 + b^3 & = (a+b)(a^2-ab+b^2) \\\\\\\\\\n& = (a+b)((a^2+2ab+b^2)-3ab) \\\\\\\\\\n& = (a+b)((a+b)^2-3ab)\\n\\\\end{align*}Now, since $a+b=10$ and $ab=17$, we have $$a^3+b^3= (a+b)((a+b)^2-3ab)=10\\\\cdot(10^2-3\\\\cdot17)=10\\\\cdot49=', 'Let $y = f(x)$. Then, $f(f(x)) = f(y) = '], 'value_label': [1, 0], 'input_ids': [[128000, 1687, 13383, 430, 400, 64, 61, 18, 36193, 61, 18, 3, 374, 279, 2694, 315, 1403, 55204, 323, 8617, 649, 387, 13605, 439, 5035, 64, 36193, 2432, 64, 61, 17, 39130, 36193, 61, 17, 8, 13244, 5659, 420, 11, 584, 617, 220, 1144, 7413, 90, 6750, 9, 534, 64, 61, 18, 489, 293, 61, 18, 612, 284, 320, 64, 36193, 2432, 64, 61, 17, 39130, 36193, 61, 17, 8, 91255, 5, 284, 320, 64, 36193, 14699, 64, 61, 17, 10, 17, 370, 36193, 61, 17, 7435, 18, 370, 8, 91255, 5, 284, 320, 64, 36193, 14699, 64, 36193, 30876, 17, 12, 18, 370, 340, 59, 408, 90, 6750, 9, 92, 7184, 11, 2533, 400, 64, 36193, 28, 605, 3, 323, 400, 370, 28, 1114, 55976, 584, 617, 27199, 64, 61, 18, 36193, 61, 18, 28, 320, 64, 36193, 14699, 64, 36193, 30876, 17, 12, 18, 370, 11992, 605, 59, 51953, 7, 605, 61, 17, 12, 18, 59, 51953, 1114, 11992, 605, 59, 51953, 2491, 28], [128000, 10267, 400, 88, 284, 282, 2120, 8, 13244, 5112, 11, 400, 69, 968, 2120, 595, 284, 282, 7166, 8, 284, 220]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [1, 0]}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# **Optionally, inspect a batch to verify masking**\n",
    "# This step is for debugging purposes and is not required for training.\n",
    "# If you choose to keep it, ensure tensors are on the correct device.\n",
    "print(tokenized_datasets[\"train\"])\n",
    "print(\"----\")\n",
    "print(tokenized_datasets[\"train\"][:2])\n",
    "print(\"----\")\n",
    "# batch = tokenized_datasets[\"train\"][:2][]\n",
    "batch = data_collator(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
       "          1012,   102],\n",
       "        [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n",
       "           102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]), 'labels': tensor([[  101,  1996,  4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,\n",
       "          1012,   102],\n",
       "        [  101,  1045,  2572,  4083,  2055, 17953,  2361,  1998,  9932,  2651,\n",
       "           102,  -100]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
